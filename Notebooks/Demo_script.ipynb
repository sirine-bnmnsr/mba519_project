{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy9ECMuqEbU8",
        "outputId": "4e92b453-4b32-4f7f-a57d-a7519d7903bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.12/dist-packages (3.38.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: db-dtypes in /usr/local/lib/python3.12/dist-packages (1.4.4)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.28.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.43.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.8.0)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (25.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: pyarrow>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from db-dtypes) (18.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.6.1)\n",
            "=== Sentiment Analysis at Scale: Customer Feedback Pipeline ===\n",
            "Initializing Spark Session with optimized configuration...\n",
            "✓ Spark 4.0.1 initialized successfully\n",
            "✓ Available cores: 100\n",
            "\n",
            "=== Loading original reviews.csv into Spark ===\n",
            "✓ Loaded 39,160 original reviews\n",
            "✓ Expanding to 10,000,000 records using Spark\n",
            "✓ Dataset expanded\n",
            "✓ Final record count: 10,000,000\n",
            "✓ Partitions: 200\n",
            "\n",
            "✓ Phase 1 Complete: Data Ingestion & Expansion\n",
            "================================================================================\n",
            "✓ Data loaded and cached in Spark\n",
            "  Total records: 39,160\n",
            "  Partitions: 2\n",
            "\n",
            "=== Sample Data ===\n",
            "+--------+-----------+----------------------------------------+----------+-----+--------------------------------------------------+-----------------------------------------+\n",
            "|   brand|review_type|                               review_id| review_ts|stars|                                   review_text_eng|                         review_title_eng|\n",
            "+--------+-----------+----------------------------------------+----------+-----+--------------------------------------------------+-----------------------------------------+\n",
            "|Brand BB|    service|rev-bc72fff0-1f8d-4562-93c7-56c9feeb67ae|2025-10-01|    5|                                              NULL|                                     NULL|\n",
            "|Brand BB|    service|rev-14b05899-4027-4a5a-9ab3-bc488e2d3327|2025-09-10|    4|                                              NULL|                                     NULL|\n",
            "|Brand BB|    service|rev-f4773f0f-db4e-4af1-86fc-734a7d4357c5|2025-08-26|    5|                                              NULL|                                     NULL|\n",
            "|Brand BB|    service|rev-d75855c3-0ce4-4549-8ada-ea14f5ba3b75|2025-08-24|    5|Perfectly chilled, fast shipping, excellent qua...|                        Perfectly chilled|\n",
            "|Brand BB|    service|rev-b34cc411-1fcc-41c0-9958-67be6a2afcb6|2025-08-24|    5|So far we've had two orders. Everything was gre...|Top selection and quality! Fast delivery!|\n",
            "+--------+-----------+----------------------------------------+----------+-----+--------------------------------------------------+-----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "=== Data Profiling ===\n",
            "root\n",
            " |-- brand: string (nullable = true)\n",
            " |-- review_type: string (nullable = true)\n",
            " |-- review_id: string (nullable = true)\n",
            " |-- review_ts: string (nullable = true)\n",
            " |-- stars: string (nullable = true)\n",
            " |-- review_text_eng: string (nullable = true)\n",
            " |-- review_title_eng: string (nullable = true)\n",
            "\n",
            "\n",
            "Summary Statistics (raw):\n",
            "+-------+------------------+\n",
            "|summary|             stars|\n",
            "+-------+------------------+\n",
            "|  count|             37300|\n",
            "|   mean| 4.750428954423593|\n",
            "| stddev|0.7234165748061677|\n",
            "|    min|                 1|\n",
            "|    25%|               5.0|\n",
            "|    50%|               5.0|\n",
            "|    75%|               5.0|\n",
            "|    max|                 5|\n",
            "+-------+------------------+\n",
            "\n",
            "\n",
            "Null value counts (raw):\n",
            "+-----+-----------+---------+---------+-----+---------------+----------------+\n",
            "|brand|review_type|review_id|review_ts|stars|review_text_eng|review_title_eng|\n",
            "+-----+-----------+---------+---------+-----+---------------+----------------+\n",
            "|    0|       1122|    15655|    13249| 1860|           3868|           25600|\n",
            "+-----+-----------+---------+---------+-----+---------------+----------------+\n",
            "\n",
            "\n",
            "=== Data Profiling ===\n",
            "root\n",
            " |-- brand: string (nullable = true)\n",
            " |-- review_type: string (nullable = true)\n",
            " |-- review_id: string (nullable = false)\n",
            " |-- review_ts: date (nullable = true)\n",
            " |-- stars: long (nullable = true)\n",
            " |-- review_text_eng: string (nullable = true)\n",
            " |-- review_title_eng: string (nullable = true)\n",
            "\n",
            "\n",
            "Summary Statistics (extanded data):\n",
            "+-------+------------------+\n",
            "|summary|             stars|\n",
            "+-------+------------------+\n",
            "|  count|          10000000|\n",
            "|   mean|         4.6065654|\n",
            "| stddev|0.8057877390803533|\n",
            "|    min|                 1|\n",
            "|    25%|                 5|\n",
            "|    50%|                 5|\n",
            "|    75%|                 5|\n",
            "|    max|                 5|\n",
            "+-------+------------------+\n",
            "\n",
            "\n",
            "Null value counts (extanded data):\n",
            "+-----+-----------+---------+---------+-----+---------------+----------------+\n",
            "|brand|review_type|review_id|review_ts|stars|review_text_eng|review_title_eng|\n",
            "+-----+-----------+---------+---------+-----+---------------+----------------+\n",
            "|    0|          0|        0|        0|    0|              0|               0|\n",
            "+-----+-----------+---------+---------+-----+---------------+----------------+\n",
            "\n",
            "\n",
            "✓ Phase 1 Complete: Data Ingestion & Expansion\n",
            "================================================================================\n",
            "Successfully loaded 39160 rows to sentiment-analysis-a.outputs.spark_df\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import json\n",
        "\n",
        "# PySpark imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# BigQuery imports\n",
        "# Install required libraries\n",
        "!pip install google-cloud-bigquery pandas db-dtypes\n",
        "!pip install -q pandas-gbq google-cloud-bigquery\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ID = \"sentiment-analysis-a\"\n",
        "DATASET = \"outputs\"\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "def pandas_to_bq(\n",
        "    pdf: pd.DataFrame,\n",
        "    table_name: str,\n",
        "    if_exists: str = \"replace\"  # \"append\" or \"replace\"\n",
        "):\n",
        "    table_id = f\"{PROJECT_ID}.{DATASET}.{table_name}\"\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition={\n",
        "            \"replace\": bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "            \"append\": bigquery.WriteDisposition.WRITE_APPEND\n",
        "        }[if_exists],\n",
        "        autodetect=True\n",
        "    )\n",
        "\n",
        "    job = client.load_table_from_dataframe(\n",
        "        pdf,\n",
        "        table_id,\n",
        "        job_config=job_config\n",
        "    )\n",
        "    job.result()\n",
        "\n",
        "    print(f\"✓ Loaded {len(pdf):,} rows → {table_id}\")\n",
        "\n",
        "\n",
        "def spark_df_to_bq(\n",
        "    spark_df,\n",
        "    table_name,\n",
        "    write_mode=\"replace\",   # \"replace\" or \"append\"\n",
        "    max_rows=2_000_000\n",
        "):\n",
        "\n",
        "\n",
        "    count = spark_df.count()\n",
        "    print(f\"Uploading {count:,} rows → {table_name}\")\n",
        "\n",
        "    if count > max_rows:\n",
        "        raise ValueError(\n",
        "            f\"Too many rows ({count:,}). \"\n",
        "            f\"Sample or aggregate before upload.\"\n",
        "        )\n",
        "\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    pdf.to_gbq(\n",
        "        destination_table=f\"{DATASET}.{table_name}\",\n",
        "        project_id=PROJECT_ID,\n",
        "        if_exists=write_mode\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Uploaded to BigQuery: {DATASET}.{table_name}\")\n",
        "\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "from google.cloud import bigquery\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=== Sentiment Analysis at Scale: Customer Feedback Pipeline ===\")\n",
        "print(\"Initializing Spark Session with optimized configuration...\")\n",
        "\n",
        "# Initialize Spark Session with performance optimizations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SentimentAnalysisAtScale\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(f\"✓ Spark {spark.version} initialized successfully\")\n",
        "print(f\"✓ Available cores: {spark.sparkContext.defaultParallelism}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE 1: DATA INGESTION & EXPANSION (SPARK-NATIVE)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Loading original reviews.csv into Spark ===\")\n",
        "\n",
        "spark_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"reviews_37k_eng.csv\")\n",
        "\n",
        "spark_df = spark_df.filter(\n",
        "    expr(\"try_cast(review_ts as date) IS NOT NULL OR review_ts IS NULL\")\n",
        ")\n",
        "\n",
        "\n",
        "base_count = spark_df.count()\n",
        "target_rows = 10_000_000\n",
        "multiplier = (target_rows // base_count) + 1\n",
        "\n",
        "print(f\"✓ Loaded {base_count:,} original reviews\")\n",
        "print(f\"✓ Expanding to {target_rows:,} records using Spark\")\n",
        "\n",
        "# Generate multiplier DataFrame\n",
        "replication_df = spark.range(multiplier)\n",
        "\n",
        "expanded_df = (\n",
        "    spark_df\n",
        "    .crossJoin(replication_df)\n",
        "    .withColumn(\n",
        "        \"review_id\",\n",
        "        concat_ws(\n",
        "            \"-\",\n",
        "            lit(\"rev\"),\n",
        "            col(\"id\"),\n",
        "            monotonically_increasing_id()\n",
        "        )\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"review_ts_clean\",\n",
        "        to_date(col(\"review_ts\"))\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"review_ts\",\n",
        "        date_add(\n",
        "            coalesce(col(\"review_ts_clean\"), current_date()),\n",
        "            - (rand() * 730).cast(\"int\")\n",
        "        )\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"stars\",\n",
        "        when(\n",
        "            rand() < 0.2,\n",
        "            greatest(\n",
        "                lit(1),\n",
        "                least(\n",
        "                    lit(5),\n",
        "                    col(\"stars\") + when(rand() < 0.5, -1).otherwise(1)\n",
        "                )\n",
        "            )\n",
        "        ).otherwise(col(\"stars\"))\n",
        "    )\n",
        "    .drop(\"review_ts_clean\", \"id\")\n",
        "    .limit(target_rows)\n",
        ")\n",
        "\n",
        "print(\"✓ Dataset expanded\")\n",
        "\n",
        "\n",
        "# NULL SANITIZATION (FIX BLANKS BEFORE TEXT GENERATION)\n",
        "\n",
        "expanded_df = expanded_df \\\n",
        "    .withColumn(\n",
        "        \"stars\",\n",
        "        when(col(\"stars\").isNull(), lit(3)).otherwise(col(\"stars\"))\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"review_type\",\n",
        "        when(col(\"review_type\").isNull(), lit(\"product\")).otherwise(col(\"review_type\"))\n",
        "    )\n",
        "\n",
        "# INTELLIGENT TEXT FILLING\n",
        "expanded_df = expanded_df \\\n",
        "    .withColumn(\n",
        "        \"review_text_eng\",\n",
        "        when(col(\"review_text_eng\").isNull() | (col(\"review_text_eng\") == \"\"),\n",
        "            when(col(\"stars\") >= 4, lit(\"Excellent product, very satisfied.\"))\n",
        "            .when(col(\"stars\") == 3, lit(\"Average product, acceptable quality.\"))\n",
        "            .otherwise(lit(\"Disappointed with product quality.\"))\n",
        "        ).otherwise(col(\"review_text_eng\"))\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"review_title_eng\",\n",
        "        when(col(\"review_title_eng\").isNull() | (col(\"review_title_eng\") == \"\"),\n",
        "            when(col(\"stars\") >= 4, lit(\"Great purchase\"))\n",
        "            .when(col(\"stars\") == 3, lit(\"Okay\"))\n",
        "            .otherwise(lit(\"Not recommended\"))\n",
        "        ).otherwise(col(\"review_title_eng\"))\n",
        "    )\n",
        "\n",
        "# INTELLIGENT TEXT FILLING (SPARK)\n",
        "\n",
        "expanded_df = expanded_df \\\n",
        "    .withColumn(\n",
        "        \"stars\",\n",
        "        when(col(\"stars\").isNull(), lit(3)).otherwise(col(\"stars\"))\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"review_type\",\n",
        "        when(col(\"review_type\").isNull(), lit(\"product\"))\n",
        "        .otherwise(col(\"review_type\"))\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"review_text_eng\",\n",
        "        when(col(\"review_text_eng\").isNull() | (col(\"review_text_eng\") == \"\"),\n",
        "            when(col(\"stars\") >= 4, lit(\"Excellent product, very satisfied.\"))\n",
        "            .when(col(\"stars\") == 3, lit(\"Average product, acceptable quality.\"))\n",
        "            .otherwise(lit(\"Disappointed with product quality.\"))\n",
        "        ).otherwise(col(\"review_text_eng\"))\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"review_title_eng\",\n",
        "        when(col(\"review_title_eng\").isNull() | (col(\"review_title_eng\") == \"\"),\n",
        "            when(col(\"stars\") >= 4, lit(\"Great purchase\"))\n",
        "            .when(col(\"stars\") == 3, lit(\"Okay\"))\n",
        "            .otherwise(lit(\"Not recommended\"))\n",
        "        ).otherwise(col(\"review_title_eng\"))\n",
        "    )\n",
        "\n",
        "\n",
        "# Repartition + cache\n",
        "expanded_df = expanded_df.repartition(200).cache()\n",
        "expanded_df.count()\n",
        "\n",
        "print(f\"✓ Final record count: {expanded_df.count():,}\")\n",
        "print(f\"✓ Partitions: {expanded_df.rdd.getNumPartitions()}\")\n",
        "print(\"\\n✓ Phase 1 Complete: Data Ingestion & Expansion\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# Cache for performance\n",
        "spark_df.cache()\n",
        "print(f\"✓ Data loaded and cached in Spark\")\n",
        "print(f\"  Total records: {spark_df.count():,}\")\n",
        "print(f\"  Partitions: {spark_df.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n=== Sample Data ===\")\n",
        "spark_df.show(5, truncate=50)\n",
        "\n",
        "# Data profiling\n",
        "print(\"\\n=== Data Profiling ===\")\n",
        "spark_df.printSchema()\n",
        "print(\"\\nSummary Statistics (raw):\")\n",
        "spark_df.select('stars').summary().show()\n",
        "\n",
        "print(\"\\nNull value counts (raw):\")\n",
        "null_counts = spark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns])\n",
        "null_counts.show()\n",
        "\n",
        "# Data profiling\n",
        "print(\"\\n=== Data Profiling ===\")\n",
        "expanded_df.printSchema()\n",
        "print(\"\\nSummary Statistics (extanded data):\")\n",
        "expanded_df.select('stars').summary().show()\n",
        "\n",
        "print(\"\\nNull value counts (extanded data):\")\n",
        "null_counts = expanded_df.select([count(when(col(c).isNull(), c)).alias(c) for c in expanded_df.columns])\n",
        "null_counts.show()\n",
        "\n",
        "\n",
        "(\n",
        "    expanded_df\n",
        "    .repartition(20)\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .csv(\"reviews_expanded_10M_20parts\")\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n✓ Phase 1 Complete: Data Ingestion & Expansion\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "#================ SAVING TO BIGQUERY =========================\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Initialize BigQuery client\n",
        "project_id = 'sentiment-analysis-a'  # Replace with your GCP project ID\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Define BigQuery dataset and table\n",
        "dataset_id = 'outputs'  # Replace with your dataset name\n",
        "table_id = 'spark_df'      # Replace with your table name\n",
        "full_table_id = f'{project_id}.{dataset_id}.{table_id}'\n",
        "\n",
        "# Configure the load job\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition='WRITE_TRUNCATE',  # Options: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY\n",
        "    autodetect=True,  # Auto-detect schema\n",
        ")\n",
        "\n",
        "# Convert PySpark DataFrame to Pandas DataFrame\n",
        "pandas_df = spark_df.toPandas()\n",
        "\n",
        "# Load DataFrame to BigQuery\n",
        "job = client.load_table_from_dataframe(pandas_df, full_table_id, job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f'Successfully loaded {job.output_rows} rows to {full_table_id}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.getConf().get(\"spark.jars.packages\")"
      ],
      "metadata": {
        "id": "KOira6METBvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PHASE 2: REAL-TIME STREAMING INGESTION - FULL 1M ROWS TO BIGQUERY\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "import shutil\n",
        "import builtins\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, when, length\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"=== PHASE 2: STREAMING 1M REVIEWS TO BIGQUERY ===\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STREAMING SCHEMA\n",
        "# =============================================================================\n",
        "streaming_schema = StructType([\n",
        "    StructField(\"brand\", StringType(), True),\n",
        "    StructField(\"review_type\", StringType(), True),\n",
        "    StructField(\"review_id\", StringType(), True),\n",
        "    StructField(\"review_ts\", StringType(), True),\n",
        "    StructField(\"stars\", IntegerType(), True),\n",
        "    StructField(\"review_text_eng\", StringType(), True),\n",
        "    StructField(\"review_title_eng\", StringType(), True),\n",
        "    StructField(\"processing_time\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DIRECTORIES\n",
        "# =============================================================================\n",
        "streaming_dir = \"/content/streaming_reviews\"\n",
        "checkpoint_dir = \"/content/streaming_checkpoint\"\n",
        "\n",
        "for d in [streaming_dir, checkpoint_dir]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Always reset checkpoint for notebook runs\n",
        "shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "print(\"✓ Created streaming directories\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PREPARE 1M ROWS\n",
        "# =============================================================================\n",
        "print(\"\\n=== Preparing 1M rows for streaming ===\")\n",
        "\n",
        "expanded_pd = expanded_df.select(\n",
        "    \"brand\",\n",
        "    \"review_type\",\n",
        "    \"review_id\",\n",
        "    \"review_ts\",\n",
        "    \"stars\",\n",
        "    \"review_text_eng\",\n",
        "    \"review_title_eng\"\n",
        ").limit(1_000_000).toPandas()\n",
        "\n",
        "print(f\"✓ Converted {len(expanded_pd):,} rows to pandas\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STREAMING DATA GENERATOR\n",
        "# =============================================================================\n",
        "def generate_streaming_reviews_full(batch_size=1000, interval=2):\n",
        "    print(\"\\n=== Starting Full Dataset Streaming ===\")\n",
        "\n",
        "    total_batches = (len(expanded_pd) + batch_size - 1) // batch_size\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_num in range(total_batches):\n",
        "        start_idx = batch_num * batch_size\n",
        "        end_idx = builtins.min(start_idx + batch_size, len(expanded_pd))\n",
        "\n",
        "        batch = expanded_pd.iloc[start_idx:end_idx].copy()\n",
        "        now = datetime.now()\n",
        "\n",
        "        batch[\"review_ts\"] = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        batch[\"processing_time\"] = now\n",
        "\n",
        "        batch.to_json(\n",
        "            f\"{streaming_dir}/batch_{batch_num:05d}.json\",\n",
        "            orient=\"records\",\n",
        "            lines=True\n",
        "        )\n",
        "\n",
        "        if (batch_num + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\n",
        "                f\"  {end_idx:,}/{len(expanded_pd):,} rows | \"\n",
        "                f\"{end_idx / elapsed:.0f} rows/sec\"\n",
        "            )\n",
        "\n",
        "        time.sleep(interval)\n",
        "\n",
        "    print(\"✓ Streaming generator finished\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# START STREAMING GENERATOR THREAD\n",
        "# =============================================================================\n",
        "streaming_thread = threading.Thread(\n",
        "    target=generate_streaming_reviews_full,\n",
        "    daemon=True\n",
        ")\n",
        "streaming_thread.start()\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SPARK STREAMING SETUP\n",
        "# =============================================================================\n",
        "print(\"\\n=== Setting up Spark Structured Streaming ===\")\n",
        "\n",
        "streaming_df = spark.readStream \\\n",
        "    .schema(streaming_schema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 2) \\\n",
        "    .json(streaming_dir)\n",
        "\n",
        "print(\"✓ Streaming DataFrame created\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# REAL-TIME TRANSFORMATIONS\n",
        "# =============================================================================\n",
        "streaming_processed = (\n",
        "    streaming_df\n",
        "    .withColumn(\"text_length\", length(col(\"review_text_eng\")))\n",
        "    .withColumn(\n",
        "        \"sentiment_label\",\n",
        "        when(col(\"stars\") >= 4, \"Positive\")\n",
        "        .when(col(\"stars\") <= 2, \"Negative\")\n",
        "        .otherwise(\"Neutral\")\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# FOREACH-BATCH BIGQUERY WRITER (CORRECT WAY)\n",
        "# =============================================================================\n",
        "def write_batch_to_bigquery(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Executed exactly once per micro-batch by Spark\n",
        "    \"\"\"\n",
        "    if batch_df.count() == 0:\n",
        "        return\n",
        "\n",
        "    pdf = batch_df.toPandas()\n",
        "\n",
        "    pandas_to_bq(\n",
        "        pdf,\n",
        "        table_name=\"phase2_streaming_reviews_full\",\n",
        "        if_exists=\"append\"\n",
        "    )\n",
        "\n",
        "    print(f\"✓ BQ batch {batch_id}: wrote {len(pdf):,} rows\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# START STREAMING QUERY\n",
        "# =============================================================================\n",
        "query = (\n",
        "    streaming_processed.writeStream\n",
        "    .foreachBatch(write_batch_to_bigquery)\n",
        "    .outputMode(\"append\")\n",
        "    .option(\"checkpointLocation\", checkpoint_dir)\n",
        "    .start()\n",
        ")\n",
        "\n",
        "print(\"✓ Streaming query started\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MONITOR PROGRESS\n",
        "# =============================================================================\n",
        "print(\"\\n=== Monitoring Stream Progress ===\")\n",
        "\n",
        "while streaming_thread.is_alive():\n",
        "    time.sleep(15)\n",
        "    print(\"  Streaming still running...\")\n",
        "\n",
        "streaming_thread.join()\n",
        "query.awaitTermination(timeout=60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMBINE BATCH + STREAMING DATA\n",
        "# =============================================================================\n",
        "print(\"\\n=== Combining Batch and Streaming Data ===\")\n",
        "\n",
        "combined_df = spark_df.unionByName(\n",
        "    final_streaming_df.select(spark_df.columns)\n",
        ")\n",
        "\n",
        "combined_df.cache()\n",
        "\n",
        "\n",
        "\n",
        "print(f\"✓ Combined dataset: {combined_df.count():,} total reviews\")\n",
        "print(f\"  Batch data: {spark_df.count():,}\")\n",
        "print(f\"  Streaming data: {streaming_count:,}\")\n",
        "\n",
        "print(\"\\n✓ Phase 2 Complete: Streaming Integration\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "print(\"\\n Full Dataset Streaming to BigQuery\")\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_I5F93WSpBr",
        "outputId": "34139888-81b9-439d-be6b-78ea33de605d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "=== PHASE 2: STREAMING 1M REVIEWS TO BIGQUERY ===\n",
            "================================================================================\n",
            "✓ Created streaming directories\n",
            "\n",
            "=== Preparing 1M rows for streaming ===\n",
            "✓ Loaded 1,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BigQuery batch 147: 1,000 rows written\n",
            "✓ Converted 1,000,000 rows to pandas\n",
            "\n",
            "=== Starting Full Dataset Streaming ===\n",
            "\n",
            "=== Setting up Spark Structured Streaming ===\n",
            "✓ Streaming DataFrame created\n",
            "✓ Streaming query started\n",
            "\n",
            "=== Monitoring Stream Progress ===\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 0: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 1: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 2: wrote 2,000 rows\n",
            "  10,000/1,000,000 rows | 551 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 3: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 4: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 5: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  20,000/1,000,000 rows | 523 rows/sec\n",
            "  Streaming still running...\n",
            "  30,000/1,000,000 rows | 514 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 6: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 7: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 8: wrote 2,000 rows\n",
            "  40,000/1,000,000 rows | 509 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 9: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 10: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 11: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 12: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 13: wrote 2,000 rows\n",
            "  50,000/1,000,000 rows | 507 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 14: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 15: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 16: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 17: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 18: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 19: wrote 2,000 rows\n",
            "  60,000/1,000,000 rows | 505 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 20: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 21: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 22: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 23: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 24: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 25: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 26: wrote 2,000 rows\n",
            "  70,000/1,000,000 rows | 504 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 27: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 28: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 29: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 30: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 31: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 32: wrote 2,000 rows\n",
            "  80,000/1,000,000 rows | 503 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 33: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 34: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 35: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 36: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 37: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 38: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 39: wrote 2,000 rows\n",
            "  90,000/1,000,000 rows | 502 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 40: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 41: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 42: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 43: wrote 2,000 rows\n",
            "  100,000/1,000,000 rows | 502 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 44: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 45: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 46: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 47: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 48: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 49: wrote 2,000 rows\n",
            "  110,000/1,000,000 rows | 501 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 50: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 51: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 52: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 53: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 54: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 55: wrote 2,000 rows\n",
            "  120,000/1,000,000 rows | 501 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 56: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 57: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 58: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 59: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 60: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 61: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 62: wrote 2,000 rows\n",
            "  130,000/1,000,000 rows | 500 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 63: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 64: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 65: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 66: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 67: wrote 2,000 rows\n",
            "  140,000/1,000,000 rows | 500 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 68: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 69: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 70: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 71: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 72: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 73: wrote 2,000 rows\n",
            "  150,000/1,000,000 rows | 500 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 74: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 75: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 76: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 77: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 78: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 79: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  160,000/1,000,000 rows | 500 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 80: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 81: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 82: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 83: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 84: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 85: wrote 2,000 rows\n",
            "  170,000/1,000,000 rows | 500 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 86: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 87: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 88: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 89: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 90: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 91: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 92: wrote 2,000 rows\n",
            "  180,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 93: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 94: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 95: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 96: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 97: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  190,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 98: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 99: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 100: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 101: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 102: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 103: wrote 2,000 rows\n",
            "  200,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 104: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 105: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 106: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 107: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 108: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 109: wrote 2,000 rows\n",
            "  210,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 110: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 111: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 112: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 113: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 114: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 115: wrote 2,000 rows\n",
            "  220,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 116: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 117: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 118: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 119: wrote 2,000 rows\n",
            "  230,000/1,000,000 rows | 499 rows/sec\n",
            "  Streaming still running...\n",
            "  240,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 120: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 121: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 122: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 123: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 124: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  250,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 125: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 126: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 127: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 128: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 129: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 130: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 131: wrote 2,000 rows\n",
            "  260,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 132: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 133: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 134: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 135: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 136: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 137: wrote 2,000 rows\n",
            "  270,000/1,000,000 rows | 499 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 138: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 139: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 140: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 141: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 142: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 143: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  280,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 144: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 145: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 146: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 147: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 148: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 149: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 150: wrote 2,000 rows\n",
            "  290,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 151: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 152: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 153: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 154: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 155: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 156: wrote 2,000 rows\n",
            "  300,000/1,000,000 rows | 498 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 157: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 158: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 159: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 160: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 161: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 162: wrote 2,000 rows\n",
            "  310,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 163: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 164: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 165: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 166: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 167: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 168: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 169: wrote 2,000 rows\n",
            "  320,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 170: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 171: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 172: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 173: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 174: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 175: wrote 2,000 rows\n",
            "  330,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 176: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 177: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 178: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 179: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 180: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 181: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 182: wrote 2,000 rows\n",
            "  340,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 183: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 184: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 185: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 186: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 187: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 188: wrote 2,000 rows\n",
            "  350,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 189: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 190: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 191: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 192: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 193: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 194: wrote 2,000 rows\n",
            "  360,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 195: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 196: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 197: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 198: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 199: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 200: wrote 2,000 rows\n",
            "  370,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 201: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 202: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 203: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 204: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 205: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 206: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 207: wrote 2,000 rows\n",
            "  380,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 208: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 209: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 210: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 211: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 212: wrote 2,000 rows\n",
            "  390,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 213: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 214: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 215: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 216: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 217: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 218: wrote 2,000 rows\n",
            "  400,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 219: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 220: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 221: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 222: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 223: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 224: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 225: wrote 2,000 rows\n",
            "  410,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 226: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 227: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 228: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 229: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 230: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 231: wrote 2,000 rows\n",
            "  420,000/1,000,000 rows | 498 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 232: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 233: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 234: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 235: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 236: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 237: wrote 2,000 rows\n",
            "  430,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 238: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 239: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 240: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 241: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 242: wrote 2,000 rows\n",
            "  440,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 243: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 244: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 245: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 246: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 247: wrote 2,000 rows\n",
            "  450,000/1,000,000 rows | 498 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 248: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 249: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 250: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 251: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 252: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 253: wrote 2,000 rows\n",
            "  460,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 254: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 255: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 256: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 257: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 258: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 259: wrote 2,000 rows\n",
            "  470,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 260: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 261: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 262: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 263: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 264: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 265: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 266: wrote 2,000 rows\n",
            "  480,000/1,000,000 rows | 498 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 267: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 268: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 269: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 270: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 271: wrote 2,000 rows\n",
            "  490,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 272: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 273: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 274: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 275: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 276: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 277: wrote 2,000 rows\n",
            "  500,000/1,000,000 rows | 498 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 278: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 279: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 280: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 281: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 282: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 283: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  510,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 284: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 285: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 286: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 287: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 288: wrote 2,000 rows\n",
            "  520,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 289: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 290: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 291: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 292: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 293: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 294: wrote 2,000 rows\n",
            "  530,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 295: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 296: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 297: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 298: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 299: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 300: wrote 2,000 rows\n",
            "  540,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 301: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 302: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 303: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 304: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 305: wrote 2,000 rows\n",
            "  550,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 306: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 307: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 308: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 309: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 310: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 311: wrote 2,000 rows\n",
            "  560,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 312: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 313: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  570,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  580,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  590,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  Streaming still running...\n",
            "  600,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  610,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  620,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  Streaming still running...\n",
            "  630,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  640,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  650,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  Streaming still running...\n",
            "  660,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  670,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  680,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  Streaming still running...\n",
            "  690,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  700,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  710,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  Streaming still running...\n",
            "  720,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  730,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  740,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  Streaming still running...\n",
            "  750,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  760,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  770,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "  Streaming still running...\n",
            "  780,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 314: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  790,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 315: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 316: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 317: wrote 2,000 rows\n",
            "  800,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 318: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 319: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 320: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 321: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 322: wrote 2,000 rows\n",
            "  810,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 323: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 324: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 325: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 326: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 327: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 328: wrote 2,000 rows\n",
            "  820,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 329: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 330: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 331: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 332: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 333: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 334: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 335: wrote 2,000 rows\n",
            "  830,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 336: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 337: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 338: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 339: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 340: wrote 2,000 rows\n",
            "  840,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 341: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 342: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 343: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 344: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 345: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 346: wrote 2,000 rows\n",
            "  850,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 347: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 348: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 349: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 350: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 351: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 352: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 353: wrote 2,000 rows\n",
            "  860,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 354: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 355: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 356: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 357: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 358: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 359: wrote 2,000 rows\n",
            "  870,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 360: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 361: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 362: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 363: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 364: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 365: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 366: wrote 2,000 rows\n",
            "  880,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 367: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 368: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 369: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 370: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 371: wrote 2,000 rows\n",
            "  890,000/1,000,000 rows | 497 rows/sec\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 372: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 373: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 374: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 375: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 376: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 377: wrote 2,000 rows\n",
            "  900,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 378: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 379: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 380: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 381: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 382: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 383: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 384: wrote 2,000 rows\n",
            "  910,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 385: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 386: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 387: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 388: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 389: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  920,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 390: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 391: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 392: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 393: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  930,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 394: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 395: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 396: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 397: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 398: wrote 2,000 rows\n",
            "  940,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 399: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 400: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 401: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 402: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 403: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 404: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "  950,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 405: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 406: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 407: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 408: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 409: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 410: wrote 2,000 rows\n",
            "  960,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 411: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 412: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 413: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 414: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 415: wrote 2,000 rows\n",
            "  970,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 416: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 417: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 418: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 419: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 420: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 421: wrote 2,000 rows\n",
            "  980,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 422: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 423: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 424: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 425: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 426: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 427: wrote 2,000 rows\n",
            "  990,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 428: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 429: wrote 2,000 rows\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 430: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 431: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 432: wrote 2,000 rows\n",
            "  1,000,000/1,000,000 rows | 497 rows/sec\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 433: wrote 2,000 rows\n",
            "✓ Streaming generator finished\n",
            "  Streaming still running...\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 434: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 435: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 436: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 437: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 438: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 439: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 440: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 441: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 442: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 443: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 444: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 445: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 446: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 447: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 448: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 449: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 450: wrote 2,000 rows\n",
            "✓ Loaded 2,000 rows → sentiment-analysis-a.outputs.phase2_streaming_reviews_full\n",
            "✓ BQ batch 451: wrote 2,000 rows\n",
            "\n",
            "✓ Phase 2 Complete: Full Dataset Streaming to BigQuery\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Processing, Cleaning & Feature Engineering Pipeline\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== PHASE 3: DATA PROCESSING & FEATURE ENGINEERING ===\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# DATA QUALITY CHECKS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Running Data Quality Checks ===\")\n",
        "\n",
        "# Check 1: Missing critical fields\n",
        "print(\"\\n1. Missing Critical Fields:\")\n",
        "critical_fields = ['brand', 'stars', 'review_text_eng']\n",
        "for field in critical_fields:\n",
        "    null_count = combined_df.filter(col(field).isNull()).count()\n",
        "    null_pct = (null_count / combined_df.count()) * 100\n",
        "    print(f\"  {field}: {null_count:,} nulls ({null_pct:.2f}%)\")\n",
        "\n",
        "# Check 2: Invalid star ratings\n",
        "print(\"\\n2. Invalid Star Ratings:\")\n",
        "invalid_stars = combined_df.filter((col('stars') < 1) | (col('stars') > 5)).count()\n",
        "print(f\"  Invalid ratings: {invalid_stars:,}\")\n",
        "\n",
        "# Check 3: Empty text reviews\n",
        "print(\"\\n3. Empty Review Text:\")\n",
        "empty_text = combined_df.filter((col('review_text_eng').isNull()) | (col('review_text_eng') == '')).count()\n",
        "print(f\"  Empty reviews: {empty_text:,}\")\n",
        "\n",
        "# Check 4: Duplicate review IDs\n",
        "print(\"\\n4. Duplicate Review IDs:\")\n",
        "total_reviews = combined_df.count()\n",
        "unique_ids = combined_df.select('review_id').distinct().count()\n",
        "duplicates = total_reviews - unique_ids\n",
        "print(f\"  Duplicates: {duplicates:,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA CLEANING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Data Cleaning Pipeline ===\")\n",
        "\n",
        "# Remove records with missing critical fields\n",
        "cleaned_df = combined_df.filter(\n",
        "    col('review_text_eng').isNotNull() &\n",
        "    (col('review_text_eng') != '') &\n",
        "    col('stars').isNotNull() &\n",
        "    col('brand').isNotNull()\n",
        ")\n",
        "\n",
        "# Filter valid star ratings\n",
        "cleaned_df = cleaned_df.filter((col('stars') >= 1) & (col('stars') <= 5))\n",
        "\n",
        "# Remove duplicates based on review_id\n",
        "cleaned_df = cleaned_df.dropDuplicates(['review_id'])\n",
        "\n",
        "print(f\"✓ Cleaned data: {cleaned_df.count():,} records\")\n",
        "print(f\"  Removed: {combined_df.count() - cleaned_df.count():,} invalid records\")\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Feature Engineering ===\")\n",
        "\n",
        "# 1. Text-based features\n",
        "print(\"\\n1. Creating text-based features...\")\n",
        "cleaned_df = cleaned_df.withColumn('text_length', length(col('review_text_eng')))\n",
        "cleaned_df = cleaned_df.withColumn('word_count', size(split(col('review_text_eng'), ' ')))\n",
        "cleaned_df = cleaned_df.withColumn('has_title', when(col('review_title_eng').isNotNull(), 1).otherwise(0))\n",
        "\n",
        "# 2. Sentiment labels (target variable)\n",
        "print(\"2. Creating sentiment labels...\")\n",
        "cleaned_df = cleaned_df.withColumn('sentiment_label',\n",
        "    when(col('stars') >= 4, 'Positive')\n",
        "    .when(col('stars') <= 2, 'Negative')\n",
        "    .otherwise('Neutral')\n",
        ")\n",
        "\n",
        "# Binary sentiment for some models\n",
        "cleaned_df = cleaned_df.withColumn('sentiment_binary',\n",
        "    when(col('stars') >= 4, 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# 3. Temporal features\n",
        "print(\"3. Creating temporal features...\")\n",
        "cleaned_df = cleaned_df.withColumn('review_date', to_date(col('review_ts')))\n",
        "cleaned_df = cleaned_df.withColumn('review_year', year(col('review_date')))\n",
        "cleaned_df = cleaned_df.withColumn('review_month', month(col('review_date')))\n",
        "cleaned_df = cleaned_df.withColumn('review_quarter', quarter(col('review_date')))\n",
        "cleaned_df = cleaned_df.withColumn('review_day_of_week', dayofweek(col('review_date')))\n",
        "\n",
        "# 4. Brand encoding\n",
        "print(\"4. Encoding categorical features...\")\n",
        "brand_indexer = StringIndexer(inputCol='brand', outputCol='brand_index')\n",
        "cleaned_df = brand_indexer.fit(cleaned_df).transform(cleaned_df)\n",
        "\n",
        "# 5. Review type encoding\n",
        "review_type_indexer = StringIndexer(inputCol='review_type', outputCol='review_type_index')\n",
        "cleaned_df = review_type_indexer.fit(cleaned_df).transform(cleaned_df)\n",
        "\n",
        "# Cache the processed data\n",
        "cleaned_df.cache()\n",
        "print(f\"\\n✓ Feature engineering complete\")\n",
        "print(f\"  Total features: {len(cleaned_df.columns)}\")\n",
        "\n",
        "# Show sample with new features\n",
        "print(\"\\n=== Sample Processed Data ===\")\n",
        "cleaned_df.select(\n",
        "    'brand', 'stars', 'sentiment_label', 'text_length',\n",
        "    'word_count', 'review_month', 'brand_index'\n",
        ").show(10)\n",
        "\n",
        "# =============================================================================\n",
        "# EXPLORATORY DATA ANALYSIS (WITH CSV OUTPUTS)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n=== Exploratory Data Analysis ===\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1. Sentiment distribution\n",
        "# ---------------------------------------------------------------------------\n",
        "sentiment_dist = cleaned_df.groupBy('sentiment_label') \\\n",
        "    .agg(count('*').alias('count')) \\\n",
        "    .withColumn('percentage', col('count') / cleaned_df.count() * 100) \\\n",
        "    .orderBy('sentiment_label')\n",
        "\n",
        "sentiment_dist.coalesce(1).write.mode(\"overwrite\").csv(\n",
        "    \"phase3_sentiment_distribution.csv\",\n",
        "    header=True\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2. Top brands\n",
        "# ---------------------------------------------------------------------------\n",
        "brand_dist = cleaned_df.groupBy('brand') \\\n",
        "    .agg(\n",
        "        count('*').alias('review_count'),\n",
        "        avg('stars').alias('avg_rating')\n",
        "    ) \\\n",
        "    .orderBy(desc('review_count')) \\\n",
        "    .limit(10)\n",
        "\n",
        "brand_dist.coalesce(1).write.mode(\"overwrite\").csv(\n",
        "    \"phase3_brand_distribution_top10.csv\",\n",
        "    header=True\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3. Monthly trends\n",
        "# ---------------------------------------------------------------------------\n",
        "monthly_dist = cleaned_df.groupBy('review_year', 'review_month') \\\n",
        "    .agg(\n",
        "        count('*').alias('count'),\n",
        "        avg('stars').alias('avg_stars')\n",
        "    ) \\\n",
        "    .orderBy('review_year', 'review_month')\n",
        "\n",
        "monthly_dist.coalesce(1).write.mode(\"overwrite\").csv(\n",
        "    \"phase3_monthly_trends.csv\",\n",
        "    header=True\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4. Text statistics\n",
        "# ---------------------------------------------------------------------------\n",
        "text_stats = cleaned_df.select(\n",
        "    'text_length', 'word_count'\n",
        ").summary()\n",
        "\n",
        "text_stats.coalesce(1).write.mode(\"overwrite\").csv(\n",
        "    \"phase3_text_statistics.csv\",\n",
        "    header=True\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5. Star distribution\n",
        "# ---------------------------------------------------------------------------\n",
        "stars_dist = cleaned_df.groupBy('stars') \\\n",
        "    .agg(count('*').alias('count')) \\\n",
        "    .withColumn('percentage', col('count') / cleaned_df.count() * 100) \\\n",
        "    .orderBy('stars')\n",
        "\n",
        "stars_dist.coalesce(1).write.mode(\"overwrite\").csv(\n",
        "    \"phase3_star_distribution.csv\",\n",
        "    header=True\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# PARTITIONING STRATEGY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n=== Implementing Partitioning Strategy ===\")\n",
        "\n",
        "partitioned_df = cleaned_df.repartition(50, 'sentiment_label')\n",
        "partitioned_df.cache()\n",
        "partitioned_df.count()\n",
        "\n",
        "partitioned_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
        "    \"phase3_partitioned_data.csv\",\n",
        "    header=True\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Phase 3 Complete: Data Processing & Feature Engineering\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final reference\n",
        "processed_df = partitioned_df\n",
        "\n",
        "\n",
        "spark_df_to_bq(\n",
        "    processed_df,\n",
        "    \"phase3_reviews_processed\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "spark_df_to_bq(\n",
        "    sentiment_dist,\n",
        "    \"phase3_sentiment_distribution\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "spark_df_to_bq(\n",
        "    brand_dist,\n",
        "    \"phase3_brand_distribution\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "spark_df_to_bq(\n",
        "    monthly_dist,\n",
        "    \"phase3_monthly_trends\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "spark_df_to_bq(\n",
        "    text_stats,\n",
        "    \"phase3_text_statistics\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "spark_df_to_bq(\n",
        "    stars_dist,\n",
        "    \"phase3_star_distribution\",\n",
        "    write_mode=\"replace\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRPv4amIery-",
        "outputId": "b4bb4e99-9235-4c1b-9fae-4414f5a1aa13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "=== PHASE 3: DATA PROCESSING & FEATURE ENGINEERING ===\n",
            "================================================================================\n",
            "\n",
            "=== Running Data Quality Checks ===\n",
            "\n",
            "1. Missing Critical Fields:\n",
            "  brand: 0 nulls (0.00%)\n",
            "  stars: 1,860 nulls (4.52%)\n",
            "  review_text_eng: 3,868 nulls (9.40%)\n",
            "\n",
            "2. Invalid Star Ratings:\n",
            "  Invalid ratings: 0\n",
            "\n",
            "3. Empty Review Text:\n",
            "  Empty reviews: 3,868\n",
            "\n",
            "4. Duplicate Review IDs:\n",
            "  Duplicates: 15,676\n",
            "\n",
            "=== Data Cleaning Pipeline ===\n",
            "✓ Cleaned data: 23,292 records\n",
            "  Removed: 17,868 invalid records\n",
            "\n",
            "=== Feature Engineering ===\n",
            "\n",
            "1. Creating text-based features...\n",
            "2. Creating sentiment labels...\n",
            "3. Creating temporal features...\n",
            "4. Encoding categorical features...\n",
            "\n",
            "✓ Feature engineering complete\n",
            "  Total features: 19\n",
            "\n",
            "=== Sample Processed Data ===\n",
            "+--------+-----+---------------+-----------+----------+------------+-----------+\n",
            "|   brand|stars|sentiment_label|text_length|word_count|review_month|brand_index|\n",
            "+--------+-----+---------------+-----------+----------+------------+-----------+\n",
            "|Brand NN|    5|       Positive|         16|         4|           7|        2.0|\n",
            "|Brand NN|    5|       Positive|         23|         4|           4|        2.0|\n",
            "|Brand HH|    5|       Positive|        112|        21|           5|        0.0|\n",
            "|Brand HH|    5|       Positive|         65|         9|           6|        0.0|\n",
            "|Brand NN|    5|       Positive|         88|        18|           8|        2.0|\n",
            "|Brand HH|    5|       Positive|         82|        12|           2|        0.0|\n",
            "|Brand NN|    5|       Positive|         84|        17|           2|        2.0|\n",
            "|Brand NN|    5|       Positive|         32|         6|           3|        2.0|\n",
            "|Brand HH|    5|       Positive|         13|         3|           4|        0.0|\n",
            "|Brand HH|    5|       Positive|         46|         6|           8|        0.0|\n",
            "+--------+-----+---------------+-----------+----------+------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "=== Exploratory Data Analysis ===\n",
            "\n",
            "=== Implementing Partitioning Strategy ===\n",
            "\n",
            "✓ Phase 3 Complete: Data Processing & Feature Engineering\n",
            "================================================================================\n",
            "Uploading 23,292 rows → phase3_reviews_processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-600735180.py:78: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  pdf.to_gbq(\n",
            "100%|██████████| 1/1 [00:00<00:00, 8719.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Uploaded to BigQuery: outputs.phase3_reviews_processed\n",
            "Uploading 3 rows → phase3_sentiment_distribution\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-600735180.py:78: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  pdf.to_gbq(\n",
            "100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Uploaded to BigQuery: outputs.phase3_sentiment_distribution\n",
            "Uploading 10 rows → phase3_brand_distribution\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-600735180.py:78: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  pdf.to_gbq(\n",
            "100%|██████████| 1/1 [00:00<00:00, 2399.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Uploaded to BigQuery: outputs.phase3_brand_distribution\n",
            "Uploading 82 rows → phase3_monthly_trends\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-600735180.py:78: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  pdf.to_gbq(\n",
            "100%|██████████| 1/1 [00:00<00:00, 11397.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Uploaded to BigQuery: outputs.phase3_monthly_trends\n",
            "Uploading 8 rows → phase3_text_statistics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-600735180.py:78: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  pdf.to_gbq(\n",
            "100%|██████████| 1/1 [00:00<00:00, 11814.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Uploaded to BigQuery: outputs.phase3_text_statistics\n",
            "Uploading 5 rows → phase3_star_distribution\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-600735180.py:78: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  pdf.to_gbq(\n",
            "100%|██████████| 1/1 [00:00<00:00, 8490.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Uploaded to BigQuery: outputs.phase3_star_distribution\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning Pipeline - Multi-Model Sentiment Classification\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== PHASE 4: MACHINE LEARNING PIPELINE ===\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, Word2Vec\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, NaiveBayes\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.ml import Pipeline\n",
        "import time\n",
        "import builtins # Import builtins to access original Python functions\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPARATION FOR ML\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Preparing Data for ML ===\")\n",
        "\n",
        "# Sample for faster training (you can use full dataset if resources allow)\n",
        "ml_sample_size = 1_000_000  # 1M records for training\n",
        "current_df_count = processed_df.count()\n",
        "# Calculate the fraction to sample, ensuring it doesn't exceed 1.0\n",
        "fraction_to_sample = builtins.min(1.0, ml_sample_size / current_df_count)\n",
        "ml_df = processed_df.sample(fraction=fraction_to_sample, seed=42)\n",
        "ml_df = ml_df.cache()\n",
        "\n",
        "print(f\"✓ ML dataset size: {ml_df.count():,} records\")\n",
        "\n",
        "# Convert sentiment labels to numeric indices\n",
        "label_indexer = StringIndexer(inputCol='sentiment_label', outputCol='label')\n",
        "ml_df = label_indexer.fit(ml_df).transform(ml_df)\n",
        "\n",
        "# Split data: 70% train, 15% validation, 15% test\n",
        "train_df, val_df, test_df = ml_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
        "\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"  Training:   {train_df.count():,} ({train_df.count()/ml_df.count()*100:.1f}%) {val_df.count()/ml_df.count()*100:.1f}%) \")\n",
        "print(f\"  Validation: {val_df.count():,} ({val_df.count()/ml_df.count()*100:.1f}%) \")\n",
        "print(f\"  Test:       {test_df.count():,} ({test_df.count()/ml_df.count()*100:.1f}%)\")\n",
        "\n",
        "# Cache splits\n",
        "train_df.cache()\n",
        "val_df.cache()\n",
        "test_df.cache()\n",
        "\n",
        "# ============================================================================\n",
        "# TEXT PROCESSING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Building Text Processing Pipeline ===\")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(inputCol='review_text_eng', outputCol='words')\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
        "\n",
        "# TF-IDF vectorization\n",
        "cv = CountVectorizer(inputCol='filtered_words', outputCol='raw_features', vocabSize=10000)\n",
        "idf = IDF(inputCol='raw_features', outputCol='tfidf_features')\n",
        "\n",
        "# Assemble all features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['tfidf_features', 'text_length', 'word_count', 'brand_index', 'review_type_index'],\n",
        "    outputCol='features',\n",
        "    handleInvalid='skip'\n",
        ")\n",
        "\n",
        "print(\"✓ Text processing pipeline created\")\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL 1: LOGISTIC REGRESSION (Baseline)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Model 1: Logistic Regression ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Build pipeline\n",
        "lr = LogisticRegression(\n",
        "    featuresCol='features',\n",
        "    labelCol='label',\n",
        "    maxIter=10,\n",
        "    regParam=0.01\n",
        ")\n",
        "\n",
        "lr_pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, assembler, lr])\n",
        "\n",
        "# Train model\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model = lr_pipeline.fit(train_df)\n",
        "lr_train_time = time.time() - start_time\n",
        "\n",
        "# Predictions\n",
        "lr_train_pred = lr_model.transform(train_df)\n",
        "lr_test_pred = lr_model.transform(test_df)\n",
        "\n",
        "# Evaluation\n",
        "evaluator_multi = MulticlassClassificationEvaluator(\n",
        "    labelCol='label',\n",
        "    predictionCol='prediction',\n",
        "    metricName='accuracy'\n",
        ")\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(\n",
        "    labelCol='label',\n",
        "    predictionCol='prediction',\n",
        "    metricName='f1'\n",
        ")\n",
        "\n",
        "lr_train_acc = evaluator_multi.evaluate(lr_train_pred)\n",
        "lr_test_acc = evaluator_multi.evaluate(lr_test_pred)\n",
        "lr_test_f1 = evaluator_f1.evaluate(lr_test_pred)\n",
        "\n",
        "print(f\"\\n✓ Logistic Regression Results:\")\n",
        "print(f\"  Training time: {lr_train_time:.2f}s\")\n",
        "print(f\"  Training accuracy: {lr_train_acc:.4f}\")\n",
        "print(f\"  Test accuracy: {lr_test_acc:.4f}\")\n",
        "print(f\"  Test F1-score: {lr_test_f1:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL 2: RANDOM FOREST (Ensemble Method)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Model 2: Random Forest Classifier ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol='features',\n",
        "    labelCol='label',\n",
        "    numTrees=20,\n",
        "    maxDepth=10,\n",
        "    seed=42,\n",
        "    maxBins=100 # Increased maxBins to handle higher cardinality categorical features\n",
        ")\n",
        "\n",
        "rf_pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, assembler, rf])\n",
        "\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = rf_pipeline.fit(train_df)\n",
        "rf_train_time = time.time() - start_time\n",
        "\n",
        "# Predictions\n",
        "rf_train_pred = rf_model.transform(train_df)\n",
        "rf_test_pred = rf_model.transform(test_df)\n",
        "\n",
        "# Evaluation\n",
        "rf_train_acc = evaluator_multi.evaluate(rf_train_pred)\n",
        "rf_test_acc = evaluator_multi.evaluate(rf_test_pred)\n",
        "rf_test_f1 = evaluator_f1.evaluate(rf_test_pred)\n",
        "\n",
        "print(f\"\\n✓ Random Forest Results:\")\n",
        "print(f\"  Training time: {rf_train_time:.2f}s\")\n",
        "print(f\"  Training accuracy: {rf_train_acc:.4f}\")\n",
        "print(f\"  Test accuracy: {rf_test_acc:.4f}\")\n",
        "print(f\"  Test F1-score: {rf_test_f1:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "rf_classifier = rf_model.stages[-1]\n",
        "feature_importance = rf_classifier.featureImportances\n",
        "print(f\"\\n  Feature importances (top 5): {feature_importance.toArray()[:5]}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL 3: NAIVE BAYES (Probabilistic)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Model 3: Naive Bayes Classifier ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "nb = NaiveBayes(\n",
        "    featuresCol='features',\n",
        "    labelCol='label',\n",
        "    smoothing=1.0\n",
        ")\n",
        "\n",
        "nb_pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, assembler, nb])\n",
        "\n",
        "print(\"Training Naive Bayes...\")\n",
        "nb_model = nb_pipeline.fit(train_df)\n",
        "nb_train_time = time.time() - start_time\n",
        "\n",
        "# Predictions\n",
        "nb_test_pred = nb_model.transform(test_df)\n",
        "\n",
        "# Evaluation\n",
        "nb_test_acc = evaluator_multi.evaluate(nb_test_pred)\n",
        "nb_test_f1 = evaluator_f1.evaluate(nb_test_pred)\n",
        "\n",
        "print(f\"\\n✓ Naive Bayes Results:\")\n",
        "print(f\"  Training time: {nb_train_time:.2f}s\")\n",
        "print(f\"  Test accuracy: {nb_test_acc:.4f}\")\n",
        "print(f\"  Test F1-score: {nb_test_f1:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETER TUNING (Random Forest)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Hyperparameter Tuning (Random Forest) ==\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create parameter grid\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [10, 20]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
        "    .build()\n",
        "\n",
        "# Cross-validator\n",
        "cv_evaluator = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')\n",
        "\n",
        "crossval = TrainValidationSplit(\n",
        "    estimator=rf_pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=cv_evaluator,\n",
        "    trainRatio=0.8,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Use smaller sample for tuning\n",
        "tuning_sample = train_df.sample(fraction=0.2, seed=42)\n",
        "print(f\"Tuning on {tuning_sample.count():,} samples...\")\n",
        "\n",
        "cv_model = crossval.fit(tuning_sample)\n",
        "tuning_time = time.time() - start_time\n",
        "\n",
        "# Best model predictions\n",
        "best_model_pred = cv_model.transform(test_df)\n",
        "best_model_acc = evaluator_multi.evaluate(best_model_pred)\n",
        "best_model_f1 = evaluator_f1.evaluate(best_model_pred)\n",
        "\n",
        "print(f\"\\n✓ Tuned Model Results:\")\n",
        "print(f\"  Tuning time: {tuning_time:.2f}s\")\n",
        "print(f\"  Best test accuracy: {best_model_acc:.4f}\")\n",
        "print(f\"  Best test F1-score: {best_model_f1:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CLUSTERING ANALYSIS (Unsupervised)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Clustering Analysis (K-Means) ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Prepare features for clustering\n",
        "clustering_pipeline = Pipeline(stages=[tokenizer, remover, cv, idf])\n",
        "clustering_features = clustering_pipeline.fit(train_df).transform(train_df)\n",
        "\n",
        "# Assemble features\n",
        "clustering_assembler = VectorAssembler(\n",
        "    inputCols=['raw_features'],\n",
        "    outputCol='features',\n",
        "    handleInvalid='skip'\n",
        ")\n",
        "clustering_data = clustering_assembler.transform(clustering_features)\n",
        "\n",
        "# Sample for clustering\n",
        "clustering_sample = clustering_data.sample(fraction=0.1, seed=42).cache()\n",
        "print(f\"Clustering on {clustering_sample.count():,} samples...\")\n",
        "\n",
        "# K-Means clustering\n",
        "kmeans = KMeans(k=3, seed=42, featuresCol='features')\n",
        "kmeans_model = kmeans.fit(clustering_sample)\n",
        "clustering_time = time.time() - start_time\n",
        "\n",
        "# Predict clusters\n",
        "clustered = kmeans_model.transform(clustering_sample)\n",
        "\n",
        "# Analyze clusters\n",
        "cluster_analysis = clustered.groupBy('prediction', 'sentiment_label') \\\n",
        "    .agg(count('*').alias('count')) \\\n",
        "    .orderBy('prediction', 'sentiment_label')\n",
        "\n",
        "print(f\"\\n✓ Clustering Results:\")\n",
        "print(f\"  Training time: {clustering_time:.2f}s\")\n",
        "print(f\"  Silhouette score: {kmeans_model.summary.trainingCost:.4f}\")\n",
        "print(\"\\nCluster distribution by sentiment:\")\n",
        "cluster_analysis.show()\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL COMPARISON SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== MODEL COMPARISON SUMMARY ===\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model_results = [\n",
        "    (\"Logistic Regression\", lr_model, lr_train_time, lr_test_acc, lr_test_f1),\n",
        "    (\"Random Forest\", rf_model, rf_train_time, rf_test_acc, rf_test_f1),\n",
        "    (\"Naive Bayes\", nb_model, nb_train_time, nb_test_acc, nb_test_f1),\n",
        "    (\"Tuned Random Forest\", cv_model, tuning_time, best_model_acc, best_model_f1)\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Model':<25} {'Train Time':<15} {'Test Acc':<12} {'F1-Score':<12}\")\n",
        "print(\"-\" * 64)\n",
        "\n",
        "best_model = None\n",
        "best_f1 = -1\n",
        "\n",
        "for name, model, train_time, acc, f1 in model_results:\n",
        "    print(f\"{name:<25} {train_time:>10.2f}s     {acc:>8.4f}     {f1:>8.4f}\")\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_model = model\n",
        "        best_model_name = name\n",
        "\n",
        "print(f\"\\n✓ Best model selected: {best_model_name} (F1 = {best_f1:.4f})\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PATH CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "PREDICTION_OUTPUT_PATH = \"outputs/full_dataset_predictions\"\n",
        "\n",
        "# Create output directory if needed (local / compatible with Spark submit)\n",
        "os.makedirs(PREDICTION_OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# RUN INFERENCE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nRunning inference on full dataset...\")\n",
        "\n",
        "full_predictions = best_model.transform(processed_df)\n",
        "\n",
        "prediction_df = full_predictions.select(\n",
        "    \"review_id\",\n",
        "    \"brand\",\n",
        "    \"stars\",\n",
        "    \"sentiment_label\",\n",
        "    col(\"prediction\").alias(\"predicted_label\"),\n",
        "    col(\"probability\").cast(StringType()).alias(\"probability\") # Convert vector to string\n",
        ")\n",
        "\n",
        "record_count = prediction_df.count()\n",
        "print(f\"✓ Inference complete on {record_count:,} records\")\n",
        "\n",
        "\n",
        "spark_df_to_bq(\n",
        "    spark_df,\n",
        "    \"spark_df\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "\n",
        "comparison_df = spark.createDataFrame(\n",
        "    comparison_data,\n",
        "    [\"model_name\", \"train_time_sec\", \"test_accuracy\", \"test_f1\"]\n",
        ")\n",
        "\n",
        "spark_df_to_bq(\n",
        "    comparison_df,\n",
        "    \"phase4_model_comparison\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "spark_df_to_bq(\n",
        "    cluster_analysis,\n",
        "    \"phase4_cluster_analysis\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "\n",
        "spark_df_to_bq(\n",
        "    prediction_df,\n",
        "    \"phase4_model_predictions\",\n",
        "    write_mode=\"replace\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"✓ Predictions saved at: {PREDICTION_OUTPUT_PATH}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJvYJRv3tT2a",
        "outputId": "9a263abd-f2e2-4511-fbd5-52737d4383a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "=== PHASE 4: MACHINE LEARNING PIPELINE ===\n",
            "================================================================================\n",
            "\n",
            "=== Preparing Data for ML ===\n",
            "✓ ML dataset size: 23,292 records\n",
            "\n",
            "Data splits:\n",
            "  Training:   16,325 (70.1%) 14.7%) \n",
            "  Validation: 3,430 (14.7%) \n",
            "  Test:       3,537 (15.2%)\n",
            "\n",
            "=== Building Text Processing Pipeline ===\n",
            "✓ Text processing pipeline created\n",
            "\n",
            "=== Model 1: Logistic Regression ===\n",
            "Training Logistic Regression...\n",
            "\n",
            "✓ Logistic Regression Results:\n",
            "  Training time: 25.52s\n",
            "  Training accuracy: 0.9846\n",
            "  Test accuracy: 0.9189\n",
            "  Test F1-score: 0.9121\n",
            "\n",
            "=== Model 2: Random Forest Classifier ===\n",
            "Training Random Forest...\n",
            "\n",
            "✓ Random Forest Results:\n",
            "  Training time: 65.94s\n",
            "  Training accuracy: 0.9151\n",
            "  Test accuracy: 0.9138\n",
            "  Test F1-score: 0.8729\n",
            "\n",
            "  Feature importances (top 5): [0.00281224 0.00258494 0.0004981  0.00075189 0.00101027]\n",
            "\n",
            "=== Model 3: Naive Bayes Classifier ===\n",
            "Training Naive Bayes...\n",
            "\n",
            "✓ Naive Bayes Results:\n",
            "  Training time: 6.45s\n",
            "  Test accuracy: 0.9005\n",
            "  Test F1-score: 0.9090\n",
            "\n",
            "=== Hyperparameter Tuning (Random Forest) ==\n",
            "Tuning on 3,282 samples...\n",
            "\n",
            "✓ Tuned Model Results:\n",
            "  Tuning time: 110.28s\n",
            "  Best test accuracy: 0.9135\n",
            "  Best test F1-score: 0.8722\n",
            "\n",
            "=== Clustering Analysis (K-Means) ===\n",
            "Clustering on 1,650 samples...\n",
            "\n",
            "✓ Clustering Results:\n",
            "  Training time: 26.97s\n",
            "  Silhouette score: 15182.4862\n",
            "\n",
            "Cluster distribution by sentiment:\n",
            "+----------+---------------+-----+\n",
            "|prediction|sentiment_label|count|\n",
            "+----------+---------------+-----+\n",
            "|         0|       Negative|   60|\n",
            "|         0|        Neutral|   59|\n",
            "|         0|       Positive| 1237|\n",
            "|         1|        Neutral|    1|\n",
            "|         2|       Negative|    6|\n",
            "|         2|        Neutral|    5|\n",
            "|         2|       Positive|  282|\n",
            "+----------+---------------+-----+\n",
            "\n",
            "\n",
            "=== MODEL COMPARISON SUMMARY ===\n",
            "================================================================================\n",
            "\n",
            "Model                     Train Time      Test Acc     F1-Score    \n",
            "----------------------------------------------------------------\n",
            "Logistic Regression            25.52s       0.9189       0.9121\n",
            "Random Forest                  65.94s       0.9138       0.8729\n",
            "Naive Bayes                     6.45s       0.9005       0.9090\n",
            "Tuned Random Forest           110.28s       0.9135       0.8722\n",
            "\n",
            "✓ Best model selected: Logistic Regression (F1 = 0.9121)\n",
            "\n",
            "Running inference on full dataset...\n",
            "✓ Inference complete on 23,292 records\n",
            "✓ Predictions saved at: outputs/full_dataset_predictions\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Analysis & Optimization\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== PHASE 6: PERFORMANCE OPTIMIZATION & BENCHMARKING ===\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import *\n",
        "import builtins # Import builtins to access original Python functions\n",
        "\n",
        "# ============================================================================\n",
        "# BENCHMARK 1: PARTITIONING IMPACT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Benchmark 1: Impact of Partitioning ===\")\n",
        "\n",
        "# Test different partition strategies\n",
        "partition_configs = [\n",
        "    (\"No Repartition\", None),\n",
        "    (\"10 Partitions\", 10),\n",
        "    (\"50 Partitions\", 50),\n",
        "    (\"100 Partitions\", 100),\n",
        "    (\"200 Partitions\", 200)\n",
        "]\n",
        "\n",
        "benchmark_results = []\n",
        "\n",
        "# Sample data for testing\n",
        "test_df = processed_df.sample(fraction=0.1, seed=42)\n",
        "test_count = test_df.count()\n",
        "print(f\"Testing on {test_count:,} records\")\n",
        "\n",
        "for config_name, num_partitions in partition_configs:\n",
        "    print(f\"\\nTesting: {config_name}\")\n",
        "\n",
        "    # Apply partitioning\n",
        "    if num_partitions:\n",
        "        test_data = test_df.repartition(num_partitions)\n",
        "    else:\n",
        "        test_data = test_df\n",
        "\n",
        "    # Benchmark: Aggregation operation\n",
        "    start_time = time.time()\n",
        "    result = test_data.groupBy('sentiment_label', 'brand') \\\n",
        "        .agg(\n",
        "            count('*').alias('count'),\n",
        "            avg('stars').alias('avg_stars'),\n",
        "            avg('text_length').alias('avg_length')\n",
        "        ).collect()\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    actual_partitions = test_data.rdd.getNumPartitions()\n",
        "\n",
        "    benchmark_results.append({\n",
        "        'Configuration': config_name,\n",
        "        'Partitions': actual_partitions,\n",
        "        'Execution_Time': execution_time,\n",
        "        'Records_Processed': test_count\n",
        "    })\n",
        "\n",
        "    print(f\"  Partitions: {actual_partitions}\")\n",
        "    print(f\"  Execution time: {execution_time:.3f}s\")\n",
        "    print(f\"  Throughput: {test_count/execution_time:,.0f} records/sec\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Partitioning Benchmark Results:\")\n",
        "print(\"=\"*70)\n",
        "benchmark_df = pd.DataFrame(benchmark_results)\n",
        "print(benchmark_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# BENCHMARK 2: CACHING STRATEGY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n=== Benchmark 2: Caching Strategy Impact ===\")\n",
        "\n",
        "cache_results = []\n",
        "\n",
        "# Test without cache\n",
        "print(\"\\nTest 1: Without caching\")\n",
        "test_data = processed_df.sample(fraction=0.05, seed=42)\n",
        "\n",
        "# Multiple operations without cache\n",
        "start_time = time.time()\n",
        "count1 = test_data.count()\n",
        "agg1 = test_data.groupBy('sentiment_label').count().collect()\n",
        "filter1 = test_data.filter(col('stars') >= 4).count()\n",
        "no_cache_time = time.time() - start_time\n",
        "\n",
        "print(f\"  Total time (3 operations): {no_cache_time:.3f}s\")\n",
        "\n",
        "# Test with cache\n",
        "print(\"\\nTest 2: With caching\")\n",
        "test_data_cached = test_data.cache()\n",
        "test_data_cached.count()  # Materialize cache\n",
        "\n",
        "start_time = time.time()\n",
        "count2 = test_data_cached.count()\n",
        "agg2 = test_data_cached.groupBy('sentiment_label').count().collect()\n",
        "filter2 = test_data_cached.filter(col('stars') >= 4).count()\n",
        "cache_time = time.time() - start_time\n",
        "\n",
        "print(f\"  Total time (3 operations): {cache_time:.3f}s\")\n",
        "print(f\"  Speedup: {no_cache_time/cache_time:.2f}x\")\n",
        "\n",
        "cache_results.append({\n",
        "    'Strategy': 'Without Cache',\n",
        "    'Time': no_cache_time,\n",
        "    'Speedup': 1.0\n",
        "})\n",
        "cache_results.append({\n",
        "    'Strategy': 'With Cache',\n",
        "    'Time': cache_time,\n",
        "    'Speedup': no_cache_time/cache_time\n",
        "})\n",
        "\n",
        "test_data_cached.unpersist()\n",
        "\n",
        "# ============================================================================\n",
        "# BENCHMARK 3: DIFFERENT PROCESSING STRATEGIES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n=== Benchmark 3: Processing Strategy Comparison ===\")\n",
        "\n",
        "strategy_results = []\n",
        "sample_data = processed_df.sample(fraction=0.05, seed=42)\n",
        "\n",
        "# Strategy 1: RDD-based processing\n",
        "print(\"\\nStrategy 1: RDD-based approach\")\n",
        "start_time = time.time()\n",
        "rdd_result = sample_data.rdd \\\n",
        "    .map(lambda row: (row['sentiment_label'], (row['stars'], 1))) \\\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
        "    .map(lambda x: (x[0], x[1][0] / x[1][1])) \\\n",
        "    .collect()\n",
        "rdd_time = time.time() - start_time\n",
        "print(f\"  Execution time: {rdd_time:.3f}s\")\n",
        "\n",
        "# Strategy 2: DataFrame API\n",
        "print(\"\\nStrategy 2: DataFrame API approach\")\n",
        "start_time = time.time()\n",
        "df_result = sample_data.groupBy('sentiment_label') \\\n",
        "    .agg(avg('stars').alias('avg_stars')) \\\n",
        "    .collect()\n",
        "df_time = time.time() - start_time\n",
        "print(f\"  Execution time: {df_time:.3f}s\")\n",
        "\n",
        "# Strategy 3: Spark SQL\n",
        "print(\"\\nStrategy 3: Spark SQL approach\")\n",
        "sample_data.createOrReplaceTempView(\"reviews_temp\")\n",
        "start_time = time.time()\n",
        "sql_result = spark.sql(\"\"\"\n",
        "    SELECT sentiment_label, AVG(stars) as avg_stars\n",
        "    FROM reviews_temp\n",
        "    GROUP BY sentiment_label\n",
        "\"\"\").collect()\n",
        "sql_time = time.time() - start_time\n",
        "print(f\"  Execution time: {sql_time:.3f}s\")\n",
        "\n",
        "strategy_results = pd.DataFrame({\n",
        "    'Strategy': ['RDD API', 'DataFrame API', 'Spark SQL'],\n",
        "    'Execution_Time': [rdd_time, df_time, sql_time],\n",
        "    'Relative_Performance': [\n",
        "        rdd_time / builtins.min(rdd_time, df_time, sql_time),\n",
        "        df_time / builtins.min(rdd_time, df_time, sql_time),\n",
        "        sql_time / builtins.min(rdd_time, df_time, sql_time)\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Processing Strategy Comparison:\")\n",
        "print(\"=\"*60)\n",
        "print(strategy_results.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# BENCHMARK 4: SCALABILITY TEST\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n=== Benchmark 4: Scalability Analysis ===\")\n",
        "\n",
        "scalability_results = []\n",
        "data_sizes = [10000, 50000, 100000, 500000, 1000000]\n",
        "\n",
        "for size in data_sizes:\n",
        "    print(f\"\\nProcessing {size:,} records...\")\n",
        "\n",
        "    # Sample data\n",
        "    scale_data = processed_df.sample(fraction=builtins.min(1.0, size/processed_df.count()), seed=42)\n",
        "    actual_size = scale_data.count()\n",
        "\n",
        "    # Benchmark operation\n",
        "    start_time = time.time()\n",
        "    result = scale_data.groupBy('sentiment_label', 'brand') \\\n",
        "        .agg(\n",
        "            count('*').alias('count'),\n",
        "            avg('stars').alias('avg_stars')\n",
        "        ).count()\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    throughput = actual_size / execution_time\n",
        "\n",
        "    scalability_results.append({\n",
        "        'Records': actual_size,\n",
        "        'Time': execution_time,\n",
        "        'Throughput': throughput\n",
        "    })\n",
        "\n",
        "    print(f\"  Actual records: {actual_size:,}\")\n",
        "    print(f\"  Execution time: {execution_time:.3f}s\")\n",
        "    print(f\"  Throughput: {throughput:,.0f} records/sec\")\n",
        "\n",
        "scalability_df = pd.DataFrame(scalability_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Scalability Test Results:\")\n",
        "print(\"=\"*60)\n",
        "print(scalability_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# BENCHMARK 5: RESOURCE UTILIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n=== Benchmark 5: Resource Utilization Analysis ===\")\n",
        "\n",
        "# Get Spark configuration\n",
        "spark_conf = spark.sparkContext.getConf().getAll()\n",
        "print(\"\\nCurrent Spark Configuration:\")\n",
        "important_configs = ['spark.driver.memory', 'spark.executor.memory',\n",
        "                     'spark.sql.shuffle.partitions', 'spark.default.parallelism']\n",
        "for key, value in spark_conf:\n",
        "    if any(config in key for config in important_configs):\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# Analyze task distribution\n",
        "print(\"\\nTask Distribution Analysis:\")\n",
        "test_data = processed_df.sample(fraction=0.1, seed=42).repartition(20)\n",
        "\n",
        "# Count records per partition\n",
        "partition_counts = test_data.rdd.mapPartitions(lambda it: [builtins.sum(1 for _ in it)]).collect()\n",
        "print(f\"  Total partitions: {len(partition_counts)}\")\n",
        "print(f\"  Min records per partition: {builtins.min(partition_counts):,}\")\n",
        "print(f\"  Max records per partition: {builtins.max(partition_counts):,}\")\n",
        "print(f\"  Avg records per partition: {builtins.sum(partition_counts)/len(partition_counts):,.0f}\")\n",
        "print(f\"  Partition imbalance ratio: {builtins.max(partition_counts)/builtins.min(partition_counts):.2f}x\")\n",
        "\n",
        "# ============================================================================\n",
        "# PERFORMANCE SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"=== PERFORMANCE OPTIMIZATION SUMMARY ===\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. PARTITIONING RECOMMENDATIONS:\")\n",
        "optimal_partitions = benchmark_df.loc[benchmark_df['Execution_Time'].idxmin()]\n",
        "print(f\"   ✓ Optimal partition count: {optimal_partitions['Partitions']}\")\n",
        "print(f\"   ✓ Best execution time: {optimal_partitions['Execution_Time']:.3f}s\")\n",
        "print(f\"   ✓ Throughput: {optimal_partitions['Records_Processed']/optimal_partitions['Execution_Time']:,.0f} records/sec\")\n",
        "\n",
        "print(\"\\n2. CACHING IMPACT:\")\n",
        "speedup = cache_results[1]['Speedup']\n",
        "print(f\"   ✓ Performance improvement: {speedup:.2f}x faster with caching\")\n",
        "print(f\"   ✓ Recommendation: Cache frequently accessed DataFrames\")\n",
        "\n",
        "print(\"\\n3. PROCESSING STRATEGY:\")\n",
        "best_strategy = strategy_results.loc[strategy_results['Execution_Time'].idxmin(), 'Strategy']\n",
        "print(f\"   ✓ Fastest approach: {best_strategy}\")\n",
        "print(f\"   ✓ Recommendation: Use DataFrame API for best Catalyst optimization\")\n",
        "\n",
        "print(\"\\n4. SCALABILITY:\")\n",
        "avg_throughput = scalability_df['Throughput'].mean()\n",
        "print(f\"   ✓ Average throughput: {avg_throughput:,.0f} records/second\")\n",
        "print(f\"   ✓ Linear scalability: {'Yes' if scalability_df['Throughput'].std() / avg_throughput < 0.3 else 'Needs optimization'}\")\n",
        "\n",
        "print(\"\\n5. RESOURCE OPTIMIZATION:\")\n",
        "print(f\"   ✓ Partition balance: {builtins.max(partition_counts)/builtins.min(partition_counts):.2f}x imbalance\")\n",
        "print(f\"   ✓ Recommendation: {'Good balance' if builtins.max(partition_counts)/builtins.min(partition_counts) < 2 else 'Consider repartitioning'}\")\n",
        "\n",
        "# Save performance results\n",
        "print(\"\\n=== Saving Performance Results ===\")\n",
        "\n",
        "benchmark_df.to_csv('performance_partitioning.csv', index=False)\n",
        "pd.DataFrame(cache_results).to_csv('performance_caching.csv', index=False)\n",
        "strategy_results.to_csv('performance_strategies.csv', index=False)\n",
        "scalability_df.to_csv('performance_scalability.csv', index=False)\n",
        "\n",
        "print(\"✓ Saved performance_partitioning.csv\")\n",
        "print(\"✓ Saved performance_caching.csv\")\n",
        "print(\"✓ Saved performance_strategies.csv\")\n",
        "print(\"✓ Saved performance_scalability.csv\")\n",
        "\n",
        "\n",
        "\n",
        "pandas_to_bq(\n",
        "    benchmark_df,\n",
        "    \"phase5_partition_benchmark\",\n",
        "    if_exists=\"replace\"\n",
        ")\n",
        "\n",
        "\n",
        "pandas_to_bq(\n",
        "    pd.DataFrame(cache_results),\n",
        "    \"phase5_caching_benchmark\",\n",
        "    if_exists=\"replace\"\n",
        ")\n",
        "\n",
        "\n",
        "pandas_to_bq(\n",
        "    strategy_results,\n",
        "    \"phase5_processing_strategies\",\n",
        "    if_exists=\"replace\"\n",
        ")\n",
        "\n",
        "\n",
        "pandas_to_bq(\n",
        "    scalability_df,\n",
        "    \"phase5_scalability\",\n",
        "    if_exists=\"replace\"\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\n✓ Phase 6 Complete: Performance Optimization & Benchmarking\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXigCIiN3dQB",
        "outputId": "131ceefc-cd58-4c3c-a0a0-f3d3a1be1367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "=== PHASE 6: PERFORMANCE OPTIMIZATION & BENCHMARKING ===\n",
            "================================================================================\n",
            "\n",
            "=== Benchmark 1: Impact of Partitioning ===\n",
            "Testing on 2,342 records\n",
            "\n",
            "Testing: No Repartition\n",
            "  Partitions: 50\n",
            "  Execution time: 0.951s\n",
            "  Throughput: 2,464 records/sec\n",
            "\n",
            "Testing: 10 Partitions\n",
            "  Partitions: 10\n",
            "  Execution time: 1.005s\n",
            "  Throughput: 2,330 records/sec\n",
            "\n",
            "Testing: 50 Partitions\n",
            "  Partitions: 50\n",
            "  Execution time: 1.559s\n",
            "  Throughput: 1,502 records/sec\n",
            "\n",
            "Testing: 100 Partitions\n",
            "  Partitions: 100\n",
            "  Execution time: 2.060s\n",
            "  Throughput: 1,137 records/sec\n",
            "\n",
            "Testing: 200 Partitions\n",
            "  Partitions: 200\n",
            "  Execution time: 1.941s\n",
            "  Throughput: 1,207 records/sec\n",
            "\n",
            "======================================================================\n",
            "Partitioning Benchmark Results:\n",
            "======================================================================\n",
            " Configuration  Partitions  Execution_Time  Records_Processed\n",
            "No Repartition          50        0.950585               2342\n",
            " 10 Partitions          10        1.005250               2342\n",
            " 50 Partitions          50        1.558914               2342\n",
            "100 Partitions         100        2.060377               2342\n",
            "200 Partitions         200        1.940519               2342\n",
            "\n",
            "\n",
            "=== Benchmark 2: Caching Strategy Impact ===\n",
            "\n",
            "Test 1: Without caching\n",
            "  Total time (3 operations): 0.879s\n",
            "\n",
            "Test 2: With caching\n",
            "  Total time (3 operations): 0.680s\n",
            "  Speedup: 1.29x\n",
            "\n",
            "\n",
            "=== Benchmark 3: Processing Strategy Comparison ===\n",
            "\n",
            "Strategy 1: RDD-based approach\n",
            "  Execution time: 20.578s\n",
            "\n",
            "Strategy 2: DataFrame API approach\n",
            "  Execution time: 1.107s\n",
            "\n",
            "Strategy 3: Spark SQL approach\n",
            "  Execution time: 1.165s\n",
            "\n",
            "============================================================\n",
            "Processing Strategy Comparison:\n",
            "============================================================\n",
            "     Strategy  Execution_Time  Relative_Performance\n",
            "      RDD API       20.577982             18.592834\n",
            "DataFrame API        1.106770              1.000000\n",
            "    Spark SQL        1.165099              1.052703\n",
            "\n",
            "\n",
            "=== Benchmark 4: Scalability Analysis ===\n",
            "\n",
            "Processing 10,000 records...\n",
            "  Actual records: 10,132\n",
            "  Execution time: 0.779s\n",
            "  Throughput: 13,000 records/sec\n",
            "\n",
            "Processing 50,000 records...\n",
            "  Actual records: 23,292\n",
            "  Execution time: 0.802s\n",
            "  Throughput: 29,058 records/sec\n",
            "\n",
            "Processing 100,000 records...\n",
            "  Actual records: 23,292\n",
            "  Execution time: 0.517s\n",
            "  Throughput: 45,071 records/sec\n",
            "\n",
            "Processing 500,000 records...\n",
            "  Actual records: 23,292\n",
            "  Execution time: 0.318s\n",
            "  Throughput: 73,202 records/sec\n",
            "\n",
            "Processing 1,000,000 records...\n",
            "  Actual records: 23,292\n",
            "  Execution time: 0.396s\n",
            "  Throughput: 58,771 records/sec\n",
            "\n",
            "============================================================\n",
            "Scalability Test Results:\n",
            "============================================================\n",
            " Records     Time   Throughput\n",
            "   10132 0.779364 13000.346028\n",
            "   23292 0.801573 29057.880878\n",
            "   23292 0.516783 45071.183717\n",
            "   23292 0.318188 73202.072245\n",
            "   23292 0.396321 58770.568774\n",
            "\n",
            "\n",
            "=== Benchmark 5: Resource Utilization Analysis ===\n",
            "\n",
            "Current Spark Configuration:\n",
            "  spark.executor.memory: 4g\n",
            "  spark.default.parallelism: 100\n",
            "  spark.sql.shuffle.partitions: 200\n",
            "  spark.driver.memory: 4g\n",
            "\n",
            "Task Distribution Analysis:\n",
            "  Total partitions: 20\n",
            "  Min records per partition: 116\n",
            "  Max records per partition: 118\n",
            "  Avg records per partition: 117\n",
            "  Partition imbalance ratio: 1.02x\n",
            "\n",
            "\n",
            "================================================================================\n",
            "=== PERFORMANCE OPTIMIZATION SUMMARY ===\n",
            "================================================================================\n",
            "\n",
            "1. PARTITIONING RECOMMENDATIONS:\n",
            "   ✓ Optimal partition count: 50\n",
            "   ✓ Best execution time: 0.951s\n",
            "   ✓ Throughput: 2,464 records/sec\n",
            "\n",
            "2. CACHING IMPACT:\n",
            "   ✓ Performance improvement: 1.29x faster with caching\n",
            "   ✓ Recommendation: Cache frequently accessed DataFrames\n",
            "\n",
            "3. PROCESSING STRATEGY:\n",
            "   ✓ Fastest approach: DataFrame API\n",
            "   ✓ Recommendation: Use DataFrame API for best Catalyst optimization\n",
            "\n",
            "4. SCALABILITY:\n",
            "   ✓ Average throughput: 43,820 records/second\n",
            "   ✓ Linear scalability: Needs optimization\n",
            "\n",
            "5. RESOURCE OPTIMIZATION:\n",
            "   ✓ Partition balance: 1.02x imbalance\n",
            "   ✓ Recommendation: Good balance\n",
            "\n",
            "=== Saving Performance Results ===\n",
            "✓ Saved performance_partitioning.csv\n",
            "✓ Saved performance_caching.csv\n",
            "✓ Saved performance_strategies.csv\n",
            "✓ Saved performance_scalability.csv\n",
            "✓ Loaded 5 rows → sentiment-analysis-a.outputs.phase5_partition_benchmark\n",
            "✓ Loaded 2 rows → sentiment-analysis-a.outputs.phase5_caching_benchmark\n",
            "✓ Loaded 3 rows → sentiment-analysis-a.outputs.phase5_processing_strategies\n",
            "✓ Loaded 5 rows → sentiment-analysis-a.outputs.phase5_scalability\n",
            "\n",
            "✓ Phase 6 Complete: Performance Optimization & Benchmarking\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}